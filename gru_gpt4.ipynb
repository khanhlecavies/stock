{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 20:38:45.287571: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-03-25 20:38:45.287601: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-03-25 20:38:45.287609: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-03-25 20:38:45.287772: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-25 20:38:45.287795: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 20:38:47.534724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 8s 49ms/step - loss: 0.1234\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0041\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0030\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0022\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0016\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0012\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 9.6736e-04\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 8.4749e-04\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 8.8009e-04\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 8.3825e-04\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 8.3172e-04\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 7.7398e-04\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 7.5852e-04\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 7.5668e-04\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 7.1901e-04\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 7.1681e-04\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 7.7598e-04\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 6.9784e-04\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 6.6105e-04\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 7.9745e-04\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 6.5429e-04\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 6.5605e-04\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 5.8925e-04\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 6.4556e-04\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 6.7047e-04\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 6.5002e-04\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 5.5544e-04\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 5.6045e-04\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 6.0072e-04\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 5.1727e-04\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 5.0165e-04\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 5.1892e-04\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 4.7905e-04\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 5.9890e-04\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 4.5559e-04\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 4.6271e-04\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 4.9017e-04\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 5.3024e-04\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 4s 30ms/step - loss: 4.6890e-04\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 4.4318e-04\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 3s 26ms/step - loss: 4.5620e-04\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 5.0231e-04\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 4.3415e-04\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 5s 38ms/step - loss: 4.9138e-04\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 5s 44ms/step - loss: 4.1365e-04\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 4.0395e-04\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 4.5366e-04\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 4.9121e-04\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 4.3327e-04\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 4.8427e-04\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 4.0363e-04\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 4.4883e-04\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 6.0904e-04\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 4.2522e-04\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 5.2338e-04\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 4.3175e-04\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 4.3828e-04\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 4.7241e-04\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 4.0717e-04\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 4.8658e-04\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 4s 32ms/step - loss: 4.0222e-04\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 4.4862e-04\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 5s 45ms/step - loss: 4.4685e-04\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 3.9783e-04\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 3.8747e-04\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 4.1998e-04\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 4.0593e-04\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 4.2546e-04\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 3.7235e-04\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 3.8544e-04\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 4.1348e-04\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 3.8990e-04\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 3.9829e-04\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 4.0959e-04\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 4.4751e-04\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 3.7976e-04\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 4.3675e-04\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 5s 38ms/step - loss: 5.7641e-04\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 4s 31ms/step - loss: 4.1026e-04\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 3.9042e-04\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 3.7517e-04\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 4.4846e-04\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 4.4925e-04\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 4s 31ms/step - loss: 5.5032e-04\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 4.0851e-04\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 4.1106e-04\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 3s 25ms/step - loss: 3.6270e-04\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 3s 28ms/step - loss: 3.8160e-04\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 4.0991e-04\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 4s 30ms/step - loss: 5.5924e-04\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 3.8311e-04\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 4.2481e-04\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 3.7697e-04\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 4.1208e-04\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 4.0792e-04\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 3.8360e-04\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 4.9775e-04\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 3.7967e-04\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 4.6069e-04\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 3.9026e-04\n",
      "1/1 [==============================] - 1s 831ms/step\n",
      "Giá mở cửa dự đoán cho ngày mai: 22388.5390625\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "# Đọc dữ liệu từ tập tin JSON\n",
    "file_path = \"hsg.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Tạo DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": pd.to_datetime(data[\"t\"], unit=\"s\"),\n",
    "        \"Open\": data[\"o\"],\n",
    "        \"High\": data[\"h\"],\n",
    "        \"Low\": data[\"l\"],\n",
    "        \"Close\": data[\"c\"],\n",
    "        \"Volume\": data[\"v\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sử dụng giá đóng cửa (Close) của ngày trước để dự đoán giá mở cửa (Open) ngày tiếp theo\n",
    "df[\"Next Open\"] = df[\"Open\"].shift(-1)\n",
    "df = df.dropna()  # Xóa bỏ hàng cuối cùng vì không có giá mở cửa cho \"ngày mai\"\n",
    "\n",
    "# Chia dữ liệu thành features (X) và target (y)\n",
    "X = df[[\"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "y = df[\"Next Open\"]\n",
    "\n",
    "# Chuẩn hóa dữ liệu\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Chia dữ liệu để lấy dữ liệu mới nhất làm input cho việc dự đoán\n",
    "X_train, X_predict = X_scaled[:-1], X_scaled[-1:]\n",
    "y_train = y_scaled[:-1]\n",
    "\n",
    "# Xây dựng mô hình GRU\n",
    "model = Sequential(\n",
    "    [\n",
    "        GRU(\n",
    "            50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "        ),\n",
    "        GRU(50),\n",
    "        Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Biên dịch mô hình\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1024, verbose=1)\n",
    "\n",
    "# Dự đoán giá mở cửa cho ngày mai\n",
    "predicted_next_open_scaled = model.predict(X_predict)\n",
    "\n",
    "# Sử dụng inverse_transform để chuyển dự đoán về cùng quy mô với dữ liệu gốc\n",
    "predicted_next_open = scaler_y.inverse_transform(predicted_next_open_scaled)\n",
    "\n",
    "print(f\"Giá mở cửa dự đoán cho ngày mai: {predicted_next_open[0][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
