{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 3s 77ms/step - loss: 0.2008\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1821\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1530\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.1047\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0412\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0172\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0283\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0101\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0126\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0129\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0075\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0057\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0058\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0034\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0031\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0025\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0017\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0015\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0012\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0013\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0012\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0012\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0012\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0011\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0011\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0011\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0011\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0011\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0011\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0011\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0011\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0011\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0011\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0011\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 9.6840e-04\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0010\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0010\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0010\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0010\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0010\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 9.3529e-04\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0010\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 9.7658e-04\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 9.7878e-04\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 9.6351e-04\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0010\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 9.8169e-04\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 9.4988e-04\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 9.8370e-04\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 9.2605e-04\n",
      "1/1 [==============================] - 1s 643ms/step\n",
      "Predicted closing price for tomorrow: [[1303.7505]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, GRU, Dropout, Dense\n",
    "\n",
    "def predict_next_day_values(data_path):\n",
    "    # Load data from JSON file\n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract features and target\n",
    "    X = np.column_stack((data['o'], data['h'], data['l'], data['c'], data['v']))\n",
    "    y = np.array(data['c'])  # Close price\n",
    "\n",
    "    # Normalize data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    y_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    # Reshape data for LSTM input (samples, timesteps, features)\n",
    "    X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1))))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(GRU(units=50)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"linear\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=1024, verbose=1)\n",
    "\n",
    "    # Predict tomorrow's closing price\n",
    "    last_data_point = X_scaled[-1].reshape(1, X_scaled.shape[1], 1)\n",
    "    predicted_price_scaled = model.predict(last_data_point)\n",
    "    predicted_price = scaler.inverse_transform(predicted_price_scaled)\n",
    "\n",
    "    return predicted_price\n",
    "\n",
    "# Example usage:\n",
    "predicted_price = predict_next_day_values('data.json')\n",
    "print(\"Predicted closing price for tomorrow:\", predicted_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochPrinter(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "\n",
    "class StockPredictor:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.models = {}\n",
    "        self.history = {}\n",
    "        self.scalers = {}\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        with open(self.data_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        X = np.column_stack((data[\"o\"], data[\"h\"], data[\"l\"], data[\"c\"], data[\"v\"]))\n",
    "\n",
    "        scalers = {}\n",
    "        for key in [\"o\", \"h\", \"l\", \"c\", \"v\"]:\n",
    "            scalers[key] = MinMaxScaler()\n",
    "            if key == \"o\":\n",
    "                y_scaled = scalers[key].fit_transform(\n",
    "                    np.array(data[key]).reshape(-1, 1)\n",
    "                )\n",
    "            else:\n",
    "                scalers[key].fit(np.array(data[key]).reshape(-1, 1))\n",
    "        self.scalers = scalers\n",
    "        X_scaled = MinMaxScaler().fit_transform(X)\n",
    "        X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train_dict, self.y_test_dict = {}, {}, {}, {}\n",
    "        for key in [\"o\", \"h\", \"l\", \"c\", \"v\"]:\n",
    "            (\n",
    "                self.X_train[key],\n",
    "                self.X_test[key],\n",
    "                self.y_train_dict[key],\n",
    "                self.y_test_dict[key],\n",
    "            ) = train_test_split(\n",
    "                X_scaled,\n",
    "                scalers[key].transform(np.array(data[key]).reshape(-1, 1)),\n",
    "                test_size=0.3,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "    def build_models(self):\n",
    "        for key in [\"o\", \"h\", \"l\", \"c\", \"v\"]:\n",
    "            model = Sequential()\n",
    "            model.add(\n",
    "                Bidirectional(\n",
    "                    GRU(\n",
    "                        units=50,\n",
    "                        return_sequences=True,\n",
    "                        input_shape=(self.X_train[key].shape[1], 1),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Bidirectional(GRU(units=50)))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(32, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "            model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"linear\"))\n",
    "            model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "            self.models[key] = model\n",
    "\n",
    "    def inverse_transform_predictions(self, predicted_values):\n",
    "        inv_predicted_values = {}\n",
    "        for key in [\"o\", \"h\", \"l\", \"c\", \"v\"]:\n",
    "            inv_predicted_values[key] = self.scalers[key].inverse_transform(\n",
    "                predicted_values[key]\n",
    "            )\n",
    "        return inv_predicted_values\n",
    "\n",
    "    def train_models(self, epochs=50, batch_size=32):\n",
    "        callbacks = [EpochPrinter()]  # Callback để in ra số lượng epoch\n",
    "        for key in [\"o\", \"h\", \"l\", \"c\", \"v\"]:\n",
    "            self.history[key] = self.models[key].fit(\n",
    "                self.X_train[key],\n",
    "                self.y_train_dict[key],\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                verbose=0,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "\n",
    "    def predict_next_day_values(self):\n",
    "        last_data_point = self.X_test[\"o\"][-1].reshape(1, self.X_test[\"o\"].shape[1], 1)\n",
    "        predicted_values = {}\n",
    "        for key in [\"o\", \"h\", \"l\", \"c\", \"v\"]:\n",
    "            predicted_values[key] = self.models[key].predict(last_data_point)\n",
    "        return self.inverse_transform_predictions(predicted_values)\n",
    "\n",
    "    def save_models(self, save_dir=\"models/\"):\n",
    "        for key, model in self.models.items():\n",
    "            model.save(f\"{save_dir}{key}_model.h5\")\n",
    "\n",
    "    def load_models(self, load_dir=\"models/\"):\n",
    "        for key in [\"o\", \"h\", \"l\", \"c\", \"v\"]:\n",
    "            self.models[key] = load_model(f\"{load_dir}{key}_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = StockPredictor(\"data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.preprocess_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.build_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.load_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 974ms/step\n",
      "1/1 [==============================] - 1s 992ms/step\n",
      "1/1 [==============================] - 1s 928ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_values = predictor.predict_next_day_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "predictor.train_models(epochs=100)\n",
    "predictor.save_models()\n",
    "predicted_values_loaded = predictor.predict_next_day_values()\n",
    "\n",
    "# In ra số lượng epoch đã hoàn thành\n",
    "for key in [\"o\", \"h\", \"l\", \"c\", \"v\"]:\n",
    "    print(f\"Epochs completed for {key}:\", len(predictor.history[key].history[\"loss\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Open price for tomorrow: [[949.3951]]\n",
      "Predicted High price for tomorrow: [[948.6303]]\n",
      "Predicted Low price for tomorrow: [[929.8655]]\n",
      "Predicted Close price for tomorrow: [[945.2864]]\n",
      "Predicted Volume for tomorrow: [[1.5815574e+08]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Open price for tomorrow:\", predicted_values[\"o\"])\n",
    "print(\"Predicted High price for tomorrow:\", predicted_values[\"h\"])\n",
    "print(\"Predicted Low price for tomorrow:\", predicted_values[\"l\"])\n",
    "print(\"Predicted Close price for tomorrow:\", predicted_values[\"c\"])\n",
    "print(\"Predicted Volume for tomorrow:\", predicted_values[\"v\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
